00 Course Description
Say you have a collection of customers with a variety of characteristics such as age, location, and financial history, and 
you wish to discover patterns and sort them into clusters. Or perhaps you have a set of texts, such as wikipedia pages, and 
you wish to segment them into categories based on their content. This is the world of unsupervised learning, called as such 
because you are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure 
from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension 
reduction to matrix factorization. In this course, you'll learn the fundamentals of unsupervised learning and implement the essential 
algorithms using scikit-learn and scipy. You will learn how to cluster, transform, visualize, and extract insights from unlabeled 
datasets, and end the course by building a recommender system to recommend popular musical artists.

1 Clustering for dataset exploration
Learn how to discover the underlying groups (or "clusters") in a dataset. By the end of this chapter, you'll be clustering companies using their stock market prices, and distinguishing different species by clustering their measurements.

Icon exercise video
Unsupervised learning
50 xp
Icon exercise mc
How many clusters?
50 xp
Icon exercise interactive
Clustering 2D points
100 xp
Icon exercise interactive
Inspect your clustering
100 xp
Icon exercise video
Evaluating a clustering
50 xp
Icon exercise interactive
How many clusters of grain?
100 xp
Icon exercise interactive
Evaluating the grain clustering
100 xp
Icon exercise video
Transforming features for better clusterings
50 xp
Icon exercise interactive
Scaling fish data for clustering
100 xp
Icon exercise interactive
Clustering the fish data
100 xp
Icon exercise interactive
Clustering stocks using KMeans
100 xp
Icon exercise interactive
Which stocks move together?
100 xp
HIDE CHAPTER DETAILS

2 Visualization with hierarchical clustering and t-SNE
In this chapter, you'll learn about two unsupervised learning techniques for data visualization, hierarchical clustering and t-SNE. Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized.

Icon exercise video
Visualizing hierarchies
50 xp
Icon exercise mc
How many merges?
50 xp
Icon exercise interactive
Hierarchical clustering of the grain data
100 xp
Icon exercise interactive
Hierarchies of stocks
100 xp
Icon exercise video
Cluster labels in hierarchical clustering
50 xp
Icon exercise mc
Which clusters are closest?
50 xp
Icon exercise interactive
Different linkage, different hierarchical clustering!
100 xp
Icon exercise mc
Intermediate clusterings
50 xp
Icon exercise interactive
Extracting the cluster labels
100 xp
Icon exercise video
t-SNE for 2-dimensional maps
50 xp
Icon exercise interactive
t-SNE visualization of grain dataset
100 xp
Icon exercise interactive
A t-SNE map of the stock market
100 xp
HIDE CHAPTER DETAILS

3 Decorrelating your data and dimension reduction
Dimension reduction summarizes a dataset using its common occuring patterns. In this chapter, you'll learn about the most fundamental of dimension reduction techniques, "Principal Component Analysis" ("PCA"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!

Icon exercise video
Visualizing the PCA transformation
50 xp
Icon exercise interactive
Correlated data in nature
100 xp
Icon exercise interactive
Decorrelating the grain measurements with PCA
100 xp
Icon exercise mc
Principal components
50 xp
Icon exercise video
Intrinsic dimension
50 xp
Icon exercise interactive
The first principal component
100 xp
Icon exercise interactive
Variance of the PCA features
100 xp
Icon exercise mc
Intrinsic dimension of the fish data
50 xp
Icon exercise video
Dimension reduction with PCA
50 xp
Icon exercise interactive
Dimension reduction of the fish measurements
100 xp
Icon exercise interactive
A tf-idf word-frequency array
100 xp
Icon exercise interactive
Clustering Wikipedia part I
100 xp
Icon exercise interactive
Clustering Wikipedia part II
100 xp
HIDE CHAPTER DETAILS

4 Discovering interpretable features
In this chapter, you'll learn about a dimension reduction technique called "Non-negative matrix factorization" ("NMF") that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics, and images in terms of commonly occurring visual patterns. You'll also learn to use NMF to build recommender systems that can find you similar articles to read, or musical artists that match your listening history!

Icon exercise video
Non-negative matrix factorization (NMF)
50 xp
Icon exercise mc
Non-negative data
50 xp
Icon exercise interactive
NMF applied to Wikipedia articles
100 xp
Icon exercise interactive
NMF features of the Wikipedia articles
100 xp
Icon exercise mc
NMF reconstructs samples
50 xp
Icon exercise video
NMF learns interpretable parts
50 xp
Icon exercise interactive
NMF learns topics of documents
100 xp
Icon exercise interactive
Explore the LED digits dataset
100 xp
Icon exercise interactive
NMF learns the parts of images
100 xp
Icon exercise interactive
PCA doesn't learn parts
100 xp
Icon exercise video
Building recommender systems using NMF
50 xp
Icon exercise interactive
Which articles are similar to 'Cristiano Ronaldo'?
100 xp
Icon exercise interactive
Recommend musical artists part I
100 xp
Icon exercise interactive
Recommend musical artists part II
100 xp
Icon exercise video
Final thoughts
50 xp
HIDE CHAPTER DETAILS


